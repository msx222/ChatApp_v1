import streamlit as st
from dotenv import load_dotenv
from langchain_core.runnables import AddableDict
from openai import OpenAI
import os

# Load .env
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ENV_PATH = os.path.join(BASE_DIR, ".env")
load_dotenv(ENV_PATH)

# =======================================
# â˜… æ³•ä»¤é©åˆåˆ¤å®šAIãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
# =======================================
from src.workflows.judgement_ai.graph import build_judgement_graph
# Chat UI Components
from ui.components.chat_display import render_chat

# =========================================================
# Streamlit Settings
# =========================================================
st.set_page_config(
    page_title="ä¿å®‰åŸºæº–AI",
    page_icon="ğŸš—",
    layout="wide"
)

# =========================================================
# ChatGPTé¢¨ï¼ˆä¸­å¤®å¹…ï¼‰
# =========================================================
st.markdown("""
<style>
.block-container {
    max-width: 900px !important;
    margin-left: auto !important;
    margin-right: auto !important;
}
</style>
""", unsafe_allow_html=True)


NAVBAR_HEIGHT = 40

# ==============================
# å›ºå®š Navbar
# ==============================
def navbar():
    st.markdown(
        f"""
        <div style="
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: {NAVBAR_HEIGHT}px;
            background-color: white;
            border-bottom: 1px solid #ddd;
            display: flex;
            align-items: center;
            padding: 0 25px;
            z-index: 9999999;
        ">
            <img src="https://www.mitsubishielectric-mobility.com/assets_gws_template_responsive/img/logo_ja.svg"
                 style="height: 35px; margin-right: 12px;">
            <span style="font-size: 19px; font-weight: 600;">
                ğŸš— å“æƒ…äºŒ æ¥­å‹™ã‚µãƒãƒ¼ãƒˆAI (PoCç‰ˆ)
            </span>
        </div>
        """,
        unsafe_allow_html=True,
    )

navbar()

# ==============================
# â˜… ã“ã“ãŒã•ã£ãã‚¨ãƒ©ãƒ¼å‡ºã¦ãŸã¨ã“ã‚ï¼ˆå®Œå…¨ç‰ˆCSSï¼‰
# ==============================
st.markdown(
    f"""
    <style>
        /* ãƒ¡ã‚¤ãƒ³å´ã‚’ä¸‹ã’ã‚‹ */
        div[data-testid="stAppViewContainer"] {{
            padding-top: {NAVBAR_HEIGHT + 1}px !important;
        }}

        section[data-testid="stMain"] {{
            padding-top: {NAVBAR_HEIGHT + 1}px !important;
        }}

        .block-container {{
            padding-top: {NAVBAR_HEIGHT + 1}px !important;
        }}


    </style>
    """,
    unsafe_allow_html=True,
)
st.markdown("""
<style>

    /* block-container ã®æœ€å¤§å¹…ã¨æƒãˆã‚‹ */
    div[data-testid="stChatInput"] {
        max-width: 740px !important;   /* Streamlitãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®block width */
        margin-left: auto !important;
        margin-right: auto !important;
    }

    textarea[data-testid="stChatInputTextArea"] {
        width: 100% !important;
    }

</style>
""", unsafe_allow_html=True)

st.markdown("""
<style>

/* ==============================
   ChatGPT é¢¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒ–ãƒ«
   ============================== */

/* Chatãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…¨ä½“ã‚³ãƒ³ãƒ†ãƒŠ */
.stChatMessage {
    padding: 0 !important;
    margin-bottom: 10px !important;
}

/* --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆå³å¯„ã›ï¼‰ --- */
.stChatMessage[data-testid="stChatMessage-user"] {
    display: flex;
    justify-content: flex-end;  /* å³å¯„ã› */
}

/* ãƒãƒ–ãƒ«æœ¬ä½“ */
.stChatMessage[data-testid="stChatMessage-user"] .stChatMessageContent {
    background: #e7f3ff;               /* ChatGPTãƒ¦ãƒ¼ã‚¶ãƒ¼è‰²(é’ç³») */
    color: #1a1a1a !important;
    padding: 10px 14px;
    border-radius: 12px;
    max-width: 75%;                    /* ChatGPTã®å¹…æ„Ÿ */
    border: 1px solid #c7e0ff;
    box-shadow: 0 1px 2px rgba(0,0,0,0.08);
}

/* ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã®ä½™ç™½èª¿æ•´ */
.stChatMessage[data-testid="stChatMessage-user"] .stMarkdown {
    margin: 0 !important;
    padding: 0 !important;
}


/* ==============================
   AI ãƒãƒ–ãƒ«ï¼ˆå·¦å¯„ã›ï¼‰
   ============================== */
.stChatMessage[data-testid="stChatMessage-assistant"] {
    display: flex;
    justify-content: flex-start;
}

.stChatMessage[data-testid="stChatMessage-assistant"] .stChatMessageContent {
    background: #ffffff;
    padding: 10px 14px;
    border-radius: 12px;
    max-width: 85%;
    border: 1px solid #eee;
    box-shadow: 0 1px 2px rgba(0,0,0,0.06);
}

.stChatMessage[data-testid="stChatMessage-assistant"] .stMarkdown {
    margin: 0 !important;
    padding: 0 !important;
}

</style>
""", unsafe_allow_html=True)



# ==============================
# ãƒãƒ£ãƒƒãƒˆå±¥æ­´
# ==============================
if "messages" not in st.session_state:
    st.session_state.messages = []

# ==============================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆãƒ¢ãƒ¼ãƒ‰é¸æŠï¼‰
# ==============================
with st.sidebar:

    mode = st.radio(
        "âœ¨AIãƒ¢ãƒ¼ãƒ‰é¸æŠ",
        ["é€šå¸¸ãƒãƒ£ãƒƒãƒˆ", "æ³•ä»¤é©åˆåˆ¤å®š", "PDFè§£æ"],
    )
    st.markdown("---")
    # mode = st.radio(
    #     "ğŸ› ï¸è‡ªå‹•åŒ–ãƒ„ãƒ¼ãƒ«",
    #     ["ç•°è­°ç”³è«‹å‡¦ç†",],
    # )


# # ==============================
# # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
# # ==============================
# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# =========================================================
# Chatè¡¨ç¤º
# =========================================================
render_chat(st.session_state.messages)



# ==============================
# å…¥åŠ›å‡¦ç†
# ==============================
#  =========================================================
# åˆ¤å®šçµæœ â†’ Markdown ã«å¤‰æ›ï¼ˆéå»äº‹ä¾‹ã¤ãï¼‰
# =========================================================
def generate_judgement_markdown(final_result):
    md = []
    overall = final_result["overall_judgement"]

    md.append(f"#### ğŸ“˜ æŠ€è¡“åŸºæº–ãƒ»é©åˆåˆ¤å®šï¼ˆç·åˆåˆ¤å®šï¼š{overall}ï¼‰\n")

    for art in final_result["articles"]:
        md.append("---\n")
        pdf = art.get("pdf_url")
        md.append(f"##### {art['article']}ï¼ˆ{art['title']}ï¼‰")
        if pdf:
            md.append(f"ğŸ”— [PDFãƒªãƒ³ã‚¯ã‚’è¦‹ã‚‹]({pdf})\n")

        for cl in art["clauses"]:
            md.append(f"###### â— {cl['clause']}ï¼š{cl['overall']}")
            for req in cl["requirements"]:
                md.append(f"""
- **R{req["req_id"]}**: {req["text"]}
    - åˆ¤å®š: {req["judgement"]}
    - ä¿¡é ¼åº¦: {req["confidence"]:.2f}
    - ç†ç”±: {req["reasoning"]}
""")

    # ---- éå»äº‹ä¾‹ï¼ˆä»Šã¯ãƒ€ãƒŸãƒ¼ï¼šå°†æ¥RAGï¼‰ ----
    md.append("---")
    md.append("#### ğŸ›  éå»ãƒ»é¡ä¼¼ä¸å…·åˆäº‹ä¾‹ï¼ˆå‚è€ƒï¼‰")

    past_cases = [
        {"year": 2022, "title": "å‰ç…§ç¯ é’è‰²ç‚¹ç¯ã®ä¸é©åˆ", "category": "ç¯ç«",
         "desc": "é’è‰²LEDãŒåŸå› ã§ä¸é©åˆã€‚"},
        {"year": 2021, "title": "åˆ¶å‹•ç¯ å…‰åº¦ä¸è¶³", "category": "ç¯ç«",
         "desc": "å…‰åº¦ãŒåŸºæº–å€¤ä¸è¶³ã§ä¸é©åˆã€‚"}
    ]

    for case in past_cases:
        md.append(f"""
##### â— {case["year"]}å¹´ã€Œ{case["title"]}ã€
- åŒºåˆ†ï¼š{case["category"]}
- å†…å®¹ï¼š{case["desc"]}
""")

    return "\n".join(md)



if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", accept_file="multiple"):

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã‚’å±¥æ­´ã¸
    # st.session_state.messages.append({"role": "user", "content": prompt})
    user_text = prompt.text if hasattr(prompt, "text") else str(prompt)

    st.session_state.messages.append({
        "role": "user",
        "content": user_text
    })
    # -------------------------
    # â˜… æ³•ä»¤é©åˆåˆ¤å®šãƒ¢ãƒ¼ãƒ‰
    # -------------------------
    if mode == "æ³•ä»¤é©åˆåˆ¤å®š":
        graph = build_judgement_graph()

        with st.spinner("é©åˆæ€§ã‚’åˆ¤å®šä¸­â€¦"):
            state = graph.invoke({"input_text": prompt})

        final = state["final_result"]
        md = generate_judgement_markdown(final)

        st.session_state.messages.append(
            {"role": "assistant", "content": md}
        )
        # â˜…â˜…â˜… ã“ã‚ŒãŒãªã„ã¨ç”»é¢ã«åæ˜ ã•ã‚Œãªã„ â˜…â˜…â˜…
        st.rerun()

        # graph = build_judgement_graph()
        #
        # with st.spinner("æ³•ä»¤é©åˆæ€§ã‚’åˆ¤å®šä¸­â€¦"):
        #     result_state = graph.invoke({"input_text": prompt})
        #
        # final = result_state["final_result"]
        #
        # # â˜… åˆ¤å®šçµæœã‚’ãã®ã¾ã¾ä¼šè©±ã«è¿½åŠ 
        # output_text = "### ğŸ“˜ æ³•ä»¤é©åˆåˆ¤å®š çµæœ\n"
        #
        # for art in final["articles"]:
        #     output_text += f"#### {art['article']}ï¼ˆ{art['title']}ï¼‰\n"
        #     for cl in art["clauses"]:
        #         output_text += f"- **{cl['clause']}ï¼š{cl['overall']}**\n"
        #         for req in cl["requirements"]:
        #             output_text += f"    - R{req['req_id']} {req['text']}\n"
        #             output_text += f"        - åˆ¤å®šï¼š{req['judgement']}\n"
        #             output_text += f"        - ä¿¡é ¼åº¦ï¼š{req['confidence']:.2f}\n"
        #             output_text += f"        - ç†ç”±ï¼š{req['reasoning']}\n"
        #
        # st.session_state.messages.append({
        #     "role": "assistant",
        #     "content": output_text
        # })

    # -------------------------
    # é€šå¸¸ãƒãƒ£ãƒƒãƒˆ / PDFè§£æ â†’ GPT å¿œç­”
    # -------------------------
    else:
        # -------------------------
        # é€šå¸¸ãƒãƒ£ãƒƒãƒˆ / PDFè§£æ â†’ GPT å¿œç­”
        # -------------------------
        from openai import OpenAI

        client = OpenAI()

        # ChatInputValue â†’ ç´”ãƒ†ã‚­ã‚¹ãƒˆã¸å¤‰æ›
        user_text = prompt.text if hasattr(prompt, "text") else str(prompt)
        with st.spinner("LLMã¸å•ã„åˆã‚ã›ä¸­â€¦"):
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": [
                            {
                                "type": "text",
                                "text": f"ã‚ãªãŸã¯ '{mode}' ãƒ¢ãƒ¼ãƒ‰ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
                            }
                        ]
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": user_text}
                        ]
                    },
                ],
                temperature=0.2,
            )
            gpt_answer = completion.choices[0].message.content

        st.session_state.messages.append(
            {"role": "assistant", "content": gpt_answer}
        )
        # â˜…â˜…â˜… ã“ã‚ŒãŒãªã„ã¨ç”»é¢ã«åæ˜ ã•ã‚Œãªã„ â˜…â˜…â˜…
        st.rerun()
    # # -------------------------
    # # é€šå¸¸ãƒãƒ£ãƒƒãƒˆ / PDFè§£æï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
    # # -------------------------
    # else:
    #     dummy_response = f"ã“ã‚Œã¯ **{mode} ãƒ¢ãƒ¼ãƒ‰** ã®ãƒ€ãƒŸãƒ¼å›ç­”ã§ã™ã€‚\n\nå…¥åŠ›: `{prompt}`"
    #     st.session_state.messages.append({"role": "assistant", "content": dummy_response})

